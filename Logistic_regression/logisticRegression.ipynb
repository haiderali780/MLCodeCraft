{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9824f4f",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bfc2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d471b55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n):\n",
    "    # Initialize the weight vector with zeros\n",
    "    w = np.zeros(n)\n",
    "    # Initialize the bias parameter to zero\n",
    "    b = 0\n",
    "    # Return the initialized weight vector and bias parameter\n",
    "    return w, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a6b088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # Compute the sigmoid function, which maps any real-valued number to the range (0, 1)\n",
    "    # The sigmoid function is defined as 1 / (1 + exp(-z)), where z is the input\n",
    "    return 1 / (1 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c008b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_and_gradients(X, y, w, b):\n",
    "    # Get the number of training examples\n",
    "    m = X.shape[0]\n",
    "    # Compute the linear combination of inputs and weights, and add bias\n",
    "    z = np.dot(X, w) + b\n",
    "    # Apply the sigmoid activation function\n",
    "    a = sigmoid(z)\n",
    "    \n",
    "    # Compute the logistic loss function\n",
    "    cost = -(1/m) * np.sum(y * np.log(a) + (1 - y) * np.log(1 - a))\n",
    "    \n",
    "    # Compute the gradients of the cost function with respect to weights and bias\n",
    "    dw = (1/m) * np.dot(X.T, (a - y))\n",
    "    db = (1/m) * np.sum(a - y)\n",
    "    \n",
    "    # Return the cost and gradients\n",
    "    return cost, dw, db\n",
    "\n",
    "def gradient_descent(X, y, w, b, learning_rate, num_iterations):\n",
    "    # List to store costs for visualization\n",
    "    costs = []\n",
    "    \n",
    "    # Loop through the specified number of iterations\n",
    "    for i in range(num_iterations):\n",
    "        # Compute cost and gradients for current parameters\n",
    "        cost, dw, db = compute_cost_and_gradients(X, y, w, b)\n",
    "        \n",
    "        # Update weights and bias using gradient descent\n",
    "        w -= learning_rate * dw\n",
    "        b -= learning_rate * db\n",
    "        \n",
    "        # Store cost for visualization and monitoring\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            # Print cost for monitoring progress\n",
    "            print(f\"Iteration {i}: Cost {cost}\")\n",
    "    \n",
    "    # Return optimized weights, bias, and costs\n",
    "    return w, b, costs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c7db4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, b):\n",
    "    # Compute the linear combination of input features and weights, and add bias\n",
    "    z = np.dot(X, w) + b\n",
    "    # Apply the sigmoid activation function to obtain probabilities\n",
    "    a = sigmoid(z)\n",
    "    # Convert probabilities to binary predictions (0 or 1) based on a threshold of 0.5\n",
    "    # If the probability is greater than or equal to 0.5, classify as 1 (True), otherwise classify as 0 (False)\n",
    "    return a >= 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e2c008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X, y, w, b):\n",
    "    # Get the minimum and maximum values for the first feature\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    # Get the minimum and maximum values for the second feature\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    # Create a grid of points using the minimum and maximum values of both features\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                         np.arange(y_min, y_max, 0.01))\n",
    "    # Predict the class labels for each point on the grid\n",
    "    Z = predict(np.c_[xx.ravel(), yy.ravel()], w, b)\n",
    "    # Reshape the predictions to match the grid shape\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot the decision boundary as a filled contour plot\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "    # Scatter plot of the training data points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o')\n",
    "    # Label for the x-axis\n",
    "    plt.xlabel('Input 1')\n",
    "    # Label for the y-axis\n",
    "    plt.ylabel('Input 2')\n",
    "    # Title for the plot\n",
    "    plt.title('Decision Boundary')\n",
    "    # Display the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7925cc5c",
   "metadata": {},
   "source": [
    "# Task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29ddea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AND gate dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c92791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "n = X.shape[1]\n",
    "w, b = initialize_parameters(n)\n",
    "\n",
    "# Train the model\n",
    "learning_rate = 0.1\n",
    "num_iterations = 1000\n",
    "w, b, costs = gradient_descent(X, y, w, b, learning_rate, num_iterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8459fbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict outputs\n",
    "predictions = predict(X, w, b)\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"Actual:\", y)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(predictions == y) * 100\n",
    "print(f\"Accuracy: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2b9fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Weights:\", w)\n",
    "print(\"Model Bias:\", b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53531f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decision boundary\n",
    "plot_decision_boundary(X, y, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f53489d",
   "metadata": {},
   "source": [
    "# Task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0265152",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# OR gate dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 1, 1, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85502698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "n = X.shape[1]\n",
    "w, b = initialize_parameters(n)\n",
    "\n",
    "# Train the model\n",
    "learning_rate = 0.1\n",
    "num_iterations = 1000\n",
    "w, b, costs = gradient_descent(X, y, w, b, learning_rate, num_iterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e2ab84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict outputs using the predict function with the trained weights and bias\n",
    "predictions = predict(X, w, b)\n",
    "\n",
    "# Print the predicted outputs and the actual labels\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"Actual:\", y)\n",
    "\n",
    "# Calculate accuracy by comparing the predicted outputs with the actual labels\n",
    "accuracy = np.mean(predictions == y) * 100\n",
    "\n",
    "# Print the accuracy percentage\n",
    "print(f\"Accuracy: {accuracy}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d45c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Weights:\", w)\n",
    "print(\"Model Bias:\", b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b806ffe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decision boundary\n",
    "plot_decision_boundary(X, y, w, b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7170cbf3",
   "metadata": {},
   "source": [
    "# Task3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6559b842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264fce9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a748cd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Class\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=10000):\n",
    "        # Initialize logistic regression with specified learning rate and number of iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Fit the logistic regression model to the training data\n",
    "        \n",
    "        # Get the number of training examples and features\n",
    "        self.m, self.n = X.shape\n",
    "        \n",
    "        # Initialize weights and bias\n",
    "        self.weights = np.zeros((self.n, 1))\n",
    "        self.bias = 0\n",
    "        \n",
    "        # Lists to store losses, accuracies, and iterations for plotting\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "        self.iterations = []\n",
    "        \n",
    "        # Iterate through the specified number of iterations\n",
    "        for i in range(self.num_iterations):\n",
    "            # Update weights and bias using gradient descent\n",
    "            self._update_weights(X, y)\n",
    "            \n",
    "            # Save loss and accuracy every 1000 iterations and at the last iteration\n",
    "            if i % 1000 == 0 or i == self.num_iterations - 1:\n",
    "                loss = self._compute_loss(X, y)\n",
    "                accuracy = self._calculate_accuracy(X, y)\n",
    "                self.losses.append(loss)\n",
    "                self.accuracies.append(accuracy)\n",
    "                self.iterations.append(i)\n",
    "                if i == self.num_iterations - 1:\n",
    "                    # Save final loss and accuracy\n",
    "                    self.final_loss = loss\n",
    "                    self.final_accuracy = accuracy\n",
    "    \n",
    "    def _update_weights(self, X, y):\n",
    "        # Update weights and bias using gradient descent\n",
    "        \n",
    "        # Compute linear model\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        \n",
    "        # Apply sigmoid activation function\n",
    "        y_predicted = sigmoid(linear_model)\n",
    "        \n",
    "        # Compute gradients\n",
    "        dw = (1 / self.m) * np.dot(X.T, (y_predicted - y))\n",
    "        db = (1 / self.m) * np.sum(y_predicted - y)\n",
    "        \n",
    "        # Update weights and bias\n",
    "        self.weights -= self.learning_rate * dw\n",
    "        self.bias -= self.learning_rate * db\n",
    "    \n",
    "    def _calculate_accuracy(self, X, y):\n",
    "        # Calculate accuracy of the model on the given data\n",
    "        \n",
    "        # Predict class labels\n",
    "        y_pred = self.predict(X)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = np.mean(y_pred == y)\n",
    "        return accuracy\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Predict class labels for the given data\n",
    "        \n",
    "        # Compute linear model\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        \n",
    "        # Apply sigmoid activation function\n",
    "        y_predicted = sigmoid(linear_model)\n",
    "        \n",
    "        # Convert probabilities to class labels\n",
    "        y_predicted_class = [1 if i > 0.5 else 0 for i in y_predicted]\n",
    "        return np.array(y_predicted_class).reshape(-1, 1)\n",
    "    \n",
    "    def _compute_loss(self, X, y):\n",
    "        # Compute the logistic loss\n",
    "        \n",
    "        # Compute linear model\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        \n",
    "        # Apply sigmoid activation function\n",
    "        y_predicted = sigmoid(linear_model)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = - (1 / self.m) * np.sum(y * np.log(y_predicted + 1e-15) + (1 - y) * np.log(1 - y_predicted + 1e-15))\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559059c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "trainX = np.loadtxt('trainX.txt')\n",
    "trainY = np.loadtxt('trainY.txt')\n",
    "testX = np.loadtxt('testX.txt')\n",
    "testY = np.loadtxt('testY.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9ca4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure Y is in the correct shape (N, 1) instead of (N,)\n",
    "trainY = trainY.reshape(-1, 1)\n",
    "testY = testY.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771e16a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to binary (0 and 1)\n",
    "trainY = np.where(trainY == 2, 0, 1)\n",
    "testY = np.where(testY == 2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46320c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "trainX = trainX / 255.0\n",
    "testX = testX / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8def74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial predictions check\n",
    "model = LogisticRegression(learning_rate=0.1, num_iterations=1)\n",
    "model.fit(trainX, trainY)\n",
    "initial_predictions = model.predict(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7be4f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluating the model with different learning rates\n",
    "etas = [0.1, 0.01, 0.001]\n",
    "num_iterations = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2ab277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each learning rate (eta) in the list of etas\n",
    "for eta in etas:\n",
    "    # Print the current learning rate being used for training\n",
    "    print(f'Training with learning rate: {eta}')\n",
    "    \n",
    "    # Create a logistic regression model instance with the current learning rate and specified number of iterations\n",
    "    model = LogisticRegression(learning_rate=eta, num_iterations=num_iterations)\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    model.fit(trainX, trainY)\n",
    "    \n",
    "    # Get the final training accuracy achieved by the model\n",
    "    train_accuracy = model.final_accuracy\n",
    "    \n",
    "    # Print the training accuracy achieved with the current learning rate\n",
    "    print(f'Learning Rate: {eta}, Training Accuracy: {train_accuracy * 100:.2f}%')\n",
    "    \n",
    "    # Plotting loss and accuracy\n",
    "    # Create a new figure for plotting\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    # Plot the loss curve in the left subplot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(model.iterations, model.losses, marker='o')  # Plot loss values over iterations\n",
    "    plt.title(f'Loss Curve (Learning Rate: {eta})')  # Set title for the subplot\n",
    "    plt.xlabel('Iterations')  # Label x-axis\n",
    "    plt.ylabel('Loss')  # Label y-axis\n",
    "    \n",
    "    # Plot the accuracy curve in the right subplot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(model.iterations, [acc * 100 for acc in model.accuracies], marker='o')  # Plot accuracy values over iterations\n",
    "    plt.title(f'Accuracy Curve (Learning Rate: {eta})')  # Set title for the subplot\n",
    "    plt.xlabel('Iterations')  # Label x-axis\n",
    "    plt.ylabel('Accuracy (%)')  # Label y-axis\n",
    "    \n",
    "    # Adjust subplot layout to prevent overlapping\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdb44b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each learning rate on the test data\n",
    "for eta in etas:\n",
    "    # Create a logistic regression model instance with the current learning rate and specified number of iterations\n",
    "    model = LogisticRegression(learning_rate=eta, num_iterations=num_iterations)\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    model.fit(trainX, trainY)\n",
    "    \n",
    "    # Calculate the accuracy of the model on the test data\n",
    "    test_accuracy = model._calculate_accuracy(testX, testY)\n",
    "    \n",
    "    # Print the test accuracy achieved with the current learning rate\n",
    "    print(f'Learning Rate: {eta}, Test Accuracy: {test_accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dbddb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
